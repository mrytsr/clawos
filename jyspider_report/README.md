# è‚¡ç¥¨ + äº¤æ˜“è®¡åˆ’ - åˆå¹¶è¡¨è®¾è®¡

**æ—¶é—´**: 2026-02-04

---

## åˆå¹¶åçš„å•è¡¨ç»“æ„ (JSON)

```json
{
  "stock": {
    "name": "æµ·é¸¥ä½å·¥",
    "heat": 70
  },
  "author": {
    "name": "é†‰é…’å°éº»é›€",
    "id": "dc6011e9b521455798f03aff075636d1"
  },
  "content": {
    "view": "çœ‹å¥½",
    "time": "2026-02-04 11:41:43",
    "logic": "æˆ¿åœ°äº§æ¾ç»‘ï¼Œå¤šåœ°æ”¿åºœæ”¶æˆ¿ï¼Œç”¨ä½œä¿éšœæˆ¿..."
  },
  "stats": {
    "likes": 0,
    "forwards": 2,
    "comments": 1
  },
  "meta": {
    "url": "/plan?pageType=search&stock_name=æµ·é¸¥ä½å·¥",
    "is_strong": false,
    "crawl_time": "2026-02-04 14:50:00"
  }
}
```

---

## å­—æ®µæ€»è§ˆ (æ‰å¹³åŒ–)

| å­—æ®µ | ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|------|
| `stock_name` | string | è‚¡ç¥¨åç§° | æµ·é¸¥ä½å·¥ |
| `stock_heat` | int | çƒ­åº¦ | 70 |
| `author_name` | string | ä½œè€…æ˜µç§° | é†‰é…’å°éº»é›€ |
| `author_id` | string | ä½œè€…ID | dc6011e9b521... |
| `view` | string | è§‚ç‚¹ | çœ‹å¥½/è°¨æ…/è¯´ä¸æ¸… |
| `publish_time` | string | å‘å¸ƒæ—¶é—´ | 2026-02-04 11:41:43 |
| `core_logic` | string | æ ¸å¿ƒé€»è¾‘å†…å®¹ | æˆ¿åœ°äº§æ¾ç»‘... |
| `likes` | int | ç‚¹èµæ•° | 0 |
| `forwards` | int | è½¬å‘æ•° | 2 |
| `comments` | int | è¯„è®ºæ•° | 1 |
| `url` | string | è¯¦æƒ…URL | /plan?pageType=search&... |
| `is_strong_logic` | bool | æ˜¯å¦å¼ºé€»è¾‘ | false |
| `crawl_time` | string | æŠ“å–æ—¶é—´ | 2026-02-04 14:50:00 |

---

## JSON ç¤ºä¾‹ (å•è¡Œ)

```json
{
  "stock_name": "æµ·é¸¥ä½å·¥",
  "stock_heat": 70,
  "author_name": "é†‰é…’å°éº»é›€",
  "author_id": "dc6011e9b521455798f03aff075636d1",
  "view": "çœ‹å¥½",
  "publish_time": "2026-02-04 11:41:43",
  "core_logic": "æˆ¿åœ°äº§æ¾ç»‘ï¼Œå¤šåœ°æ”¿åºœæ”¶æˆ¿ï¼Œç”¨ä½œä¿éšœæˆ¿ã€‚åˆ©å¥½ç²¾è£…ä¿®...",
  "likes": 0,
  "forwards": 2,
  "comments": 1,
  "url": "/plan?pageType=search&stock_name=æµ·é¸¥ä½å·¥",
  "is_strong_logic": false,
  "crawl_time": "2026-02-04 14:50:00"
}
```

---

## SQLite è¡¨ç»“æ„

```sql
CREATE TABLE stocks (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    
    -- è‚¡ç¥¨
    stock_name TEXT NOT NULL,
    stock_heat INTEGER DEFAULT 0,
    
    -- ä½œè€…
    author_name TEXT,
    author_id TEXT,
    
    -- å†…å®¹
    view TEXT,
    publish_time TEXT,
    core_logic TEXT,
    
    -- äº’åŠ¨
    likes INTEGER DEFAULT 0,
    forwards INTEGER DEFAULT 0,
    comments INTEGER DEFAULT 0,
    
    -- å…ƒæ•°æ®
    url TEXT,
    is_strong_logic INTEGER DEFAULT 0,
    crawl_time TEXT,
    
    UNIQUE(stock_name, author_name, publish_time)
);

CREATE INDEX idx_stock ON stocks(stock_name);
CREATE INDEX idx_time ON stocks(publish_time DESC);
CREATE INDEX idx_heat ON stocks(stock_heat DESC);
```

---

## åˆå¹¶åŸå› 

| ä¹‹å‰ | ä¹‹å |
|------|------|
| stocks + authors + plans (3å¼ è¡¨) | stocks (1å¼ è¡¨) |
| å¤–é”®å…³è” | æ‰å¹³åŒ– |
| å¤æ‚æŸ¥è¯¢ | ç›´æ¥æŸ¥è¯¢ |
| éœ€è¦ JOIN | å•è¡¨æå®š |

---

## çˆ¬è™«ä»£ç  (åˆå¹¶ç‰ˆ)

```python
import asyncio, re, sqlite3, json
from datetime import datetime

DB = "stocks.db"

def init_db():
    with sqlite3.connect(DB) as c:
        c.execute("""CREATE TABLE IF NOT EXISTS stocks (
            id INTEGER PRIMARY KEY,
            stock_name, stock_heat, author_name, author_id,
            view, publish_time, core_logic,
            likes, forwards, comments,
            url, is_strong_logic, crawl_time,
            UNIQUE(stock_name, author_name, publish_time)
        )""")

async def crawl(pages=10):
    from browser import AgentBrowser
    browser = AgentBrowser()
    
    for p in range(1, pages+1):
        url = f"https://www.jiuyangongshe.com/plan?page={p}"
        await browser.open(url)
        snapshot = await browser.snapshot()
        
        for row in parse(snapshot):
            save(row)
        print(f"Page {p}: OK")
        await asyncio.sleep(2)

def parse(text):
    lines, results, cur = text.split('\n'), [], {}
    for l in lines:
        if 'listitem:' in l:
            if cur: results.append(cur)
            cur = {"crawl_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
        elif cur:
            if m := re.search(r'text: "(\d+)"', l): pass  # åºå·å¿½ç•¥
            if 'link "' in l and '/u/' not in l and m := re.search(r'link "([^"]+)"', l):
                cur["stock_name"] = m.group(1)
            if m := re.search(r'ğŸ”¥(\d+)', l): cur["stock_heat"] = int(m.group(1))
            if '/u/' in l:
                if m := re.search(r'link "([^"]+)"', l): cur["author_name"] = m.group(1)
                if m := re.search(r'/u/([^"\s]+)', l): cur["author_id"] = m.group(1)
            if any(x in l for x in ['2025-', '2026-']):
                if m := re.search(r'(2025-\d{2}-\d{2}|2026-\d{2}-\d{2})', l):
                    cur["publish_time"] = m.group(1)
                cur["view"] = "çœ‹å¥½" if ' çœ‹å¥½' in l else "è°¨æ…" if ' è°¨æ…' in l else "è¯´ä¸æ¸…"
                cur["core_logic"] = (l.split(' çœ‹å¥½')[-1] or l.split(' è°¨æ…')[-1] or "")[:500]
    return results + [cur] if cur else results

def save(row):
    with sqlite3.connect(DB) as c:
        c.execute("""INSERT OR REPLACE INTO stocks VALUES (
            NULL, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
        )""", (
            row.get("stock_name"), row.get("stock_heat"),
            row.get("author_name"), row.get("author_id"),
            row.get("view"), row.get("publish_time"), (row.get("core_logic") or "")[:1000],
            0, 0, 0, "", 0, row.get("crawl_time")
        ))

def export(f="stocks.json"):
    with sqlite3.connect(DB) as c:
        json.dump([dict(r) for r in c.execute("SELECT * FROM stocks ORDER BY publish_time DESC")], 
                  open(f, 'w', encoding='utf-8'))

if __name__ == "__main__":
    init_db()
    asyncio.run(crawl(10))
    export()
```

---

## ç»“è®º

âœ… **ä¸€å¼ è¡¨æå®šæ‰€æœ‰æ•°æ®**
- è‚¡ç¥¨ + ä½œè€… + äº¤æ˜“è®¡åˆ’
- æ‰å¹³åŒ–è®¾è®¡
- ç®€å•æŸ¥è¯¢
- æ˜“äºå¯¼å‡º