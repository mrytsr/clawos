# ä¹é˜³å®«äº¤æ˜“è®¡åˆ’çˆ¬è™« - ç›´æ¥ä¿å­˜ Excel

import asyncio, re, json
from datetime import datetime
from openpyxl import Workbook
from openpyxl.styles import Font, Alignment, PatternFill

EXCEL_FILE = "stocks.xlsx"
JSON_FILE = "stocks.json"

HEADERS = [
    "è‚¡ç¥¨åç§°", "çƒ­åº¦", "ä½œè€…", "ä½œè€…ID", 
    "è§‚ç‚¹", "å‘å¸ƒæ—¶é—´", "æ ¸å¿ƒé€»è¾‘",
    "ç‚¹èµ", "è½¬å‘", "è¯„è®º",
    "URL", "æ˜¯å¦å¼ºé€»è¾‘", "æŠ“å–æ—¶é—´"
]

async def crawl(pages=10):
    from browser import AgentBrowser
    browser = AgentBrowser()
    all_data = []
    
    for p in range(1, pages + 1):
        url = f"https://www.jiuyangongshe.com/plan?page={p}"
        await browser.open(url)
        snapshot = await browser.snapshot()
        
        rows = parse_snapshot(snapshot)
        for row in rows:
            all_data.append(row)
        
        print(f"Page {p}: {len(rows)} æ¡")
        await asyncio.sleep(2)
    
    # ä¿å­˜
    save_excel(all_data, EXCEL_FILE)
    print(f"\nâœ… Excel: {EXCEL_FILE}")
    
    with open(JSON_FILE, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… JSON: {JSON_FILE}")
    
    return all_data

def parse_snapshot(text):
    """è§£æ snapshot æ–‡æœ¬"""
    lines = text.split('\n')
    results = []
    current = None
    
    for line in lines:
        stripped = line.strip()
        
        # æ–°åˆ—è¡¨é¡¹å¼€å§‹
        if 'listitem:' in stripped:
            if current and current.get('stock_name'):
                results.append(current)
            current = {
                'crawl_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            continue
        
        if current is None:
            continue
        
        # è‚¡ç¥¨åç§°: link åè·Ÿè‚¡ç¥¨å (ä¸åœ¨ /u/ è·¯å¾„ä¸­)
        if 'link "' in stripped and '/u/' not in stripped:
            match = re.search(r'link "([^"]+)"', stripped)
            if match:
                name = match.group(1).strip()
                # æ’é™¤çº¯æ•°å­—åºå·å’Œç‰¹æ®Šæ–‡æœ¬
                if name and not name.isdigit() and '...' not in name:
                    current['stock_name'] = name
        
        # çƒ­åº¦
        if 'ğŸ”¥' in stripped:
            match = re.search(r'ğŸ”¥(\d+)', stripped)
            if match:
                current['stock_heat'] = int(match.group(1))
        
        # ä½œè€…: link åœ¨ /u/ è·¯å¾„ä¸­
        if '/u/' in stripped:
            match = re.search(r'link "([^"]+)"', stripped)
            if match:
                current['author_name'] = match.group(1).strip()
            u_match = re.search(r'/u/([^"\s]+)', stripped)
            if u_match:
                current['author_id'] = u_match.group(1)
        
        # æ—¶é—´ã€è§‚ç‚¹ã€å†…å®¹ (2025 æˆ– 2026 å¹´ä»½)
        if any(x in stripped for x in ['2025-', '2026-']):
            # æå–æ—¶é—´
            time_match = re.search(r'(2025-\d{2}-\d{2}|2026-\d{2}-\d{2})', stripped)
            if time_match:
                current['publish_time'] = time_match.group(1)
            
            # æå–è§‚ç‚¹
            if ' çœ‹å¥½' in stripped:
                current['view'] = 'çœ‹å¥½'
            elif ' è°¨æ…' in stripped:
                current['view'] = 'è°¨æ…'
            elif 'è¯´ä¸æ¸…' in stripped:
                current['view'] = 'è¯´ä¸æ¸…'
            
            # æå–æ ¸å¿ƒé€»è¾‘ - å–æ•´è¡Œå†…å®¹
            content = stripped
            # æ¸…ç†ä¸éœ€è¦çš„éƒ¨åˆ†
            for prefix in ['link "', '/url: ']:
                content = content.replace(prefix, '', 1)
            # æå–æ ¸å¿ƒé€»è¾‘ (æ—¶é—´åé¢çš„å†…å®¹)
            if '2026-' in content:
                content = content.split('2026-')[-1]
            elif '2025-' in content:
                content = content.split('2025-')[-1]
            # æ¸…ç†è§‚ç‚¹å‰ç¼€
            for view in ['çœ‹å¥½', 'è°¨æ…', 'è¯´ä¸æ¸…']:
                if content.startswith(view):
                    content = content[len(view):]
                    break
            current['core_logic'] = content.strip()[:500]
    
    # ä¿å­˜æœ€åä¸€æ¡
    if current and current.get('stock_name'):
        results.append(current)
    
    return results

def save_excel(data, filename):
    wb = Workbook()
    ws = wb.active
    ws.title = "äº¤æ˜“è®¡åˆ’"
    
    # è¡¨å¤´æ ·å¼
    header_font = Font(bold=True, color="FFFFFF")
    header_fill = PatternFill(start_color="07C160", end_color="07C160", fill_type="solid")
    
    # å†™è¡¨å¤´
    for col, header in enumerate(HEADERS, 1):
        cell = ws.cell(row=1, column=col, value=header)
        cell.font = header_font
        cell.fill = header_fill
        cell.alignment = Alignment(horizontal='center', vertical='center')
    
    # å†™æ•°æ®
    for row_idx, row_data in enumerate(data, 2):
        for col_idx, header in enumerate(HEADERS, 1):
            key = header_to_key(header)
            value = row_data.get(key, "")
            cell = ws.cell(row=row_idx, column=col_idx, value=value)
            cell.alignment = Alignment(wrap_text=True, vertical='top')
    
    # è®¾ç½®åˆ—å®½
    widths = [15, 8, 15, 35, 10, 18, 80, 8, 8, 8, 50, 12, 20]
    for col, width in enumerate(widths, 1):
        ws.column_dimensions[get_column_letter(col)].width = width
    
    wb.save(filename)

def header_to_key(header):
    return {
        "è‚¡ç¥¨åç§°": "stock_name",
        "çƒ­åº¦": "stock_heat",
        "ä½œè€…": "author_name",
        "ä½œè€…ID": "author_id",
        "è§‚ç‚¹": "view",
        "å‘å¸ƒæ—¶é—´": "publish_time",
        "æ ¸å¿ƒé€»è¾‘": "core_logic",
        "ç‚¹èµ": "likes",
        "è½¬å‘": "forwards",
        "è¯„è®º": "comments",
        "URL": "url",
        "æ˜¯å¦å¼ºé€»è¾‘": "is_strong_logic",
        "æŠ“å–æ—¶é—´": "crawl_time"
    }.get(header, header)

if __name__ == "__main__":
    asyncio.run(crawl(10))
